{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quasi Tracheostomy\n",
    "\n",
    "This notebook introduces the experimental set-up that we used to bring regular Czech voices closer to that of Jasmi.\n",
    "\n",
    "## Setting Up\n",
    "\n",
    "Let's start by importing libraries. We will need [pyannote/embedding](https://huggingface.co/pyannote/embedding) model that requires granted access from the model providers (for details see the [Hugging Face page](https://huggingface.co/pyannote/embedding)). In order to use this notebook successfully, ask for the access, generate a HuggingFace authentication token and store it in `HUGGING_FACE_TOKEN` variable of the [personal_settings.py](personal_settings.py) file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa, os, csv, librosa, io, scipy, torch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from io import BytesIO\n",
    "from data_processing.tokenize_audio import load_audio\n",
    "from IPython.display import Audio\n",
    "import soundfile as sf\n",
    "from pydub import AudioSegment\n",
    "from pyannote.audio import Model\n",
    "from pyannote.audio import Inference\n",
    "from personal_settings import HUGGING_FACE_TOKEN\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import entropy\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from asr.whisper_config import REGULAR_SPEECH_DIR, PATIENT_SEGMENTED_AUDIO_DIR\n",
    "\n",
    "model = Model.from_pretrained(\"pyannote/embedding\", \n",
    "                              use_auth_token=HUGGING_FACE_TOKEN)\n",
    "inference = Inference(model, window=\"whole\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And also define several functions that will help us later"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_entropy(classes, labels):\n",
    "    \"\"\"\n",
    "    Compute entropy for each cluster's class distribution.\n",
    "    \"\"\"\n",
    "    unique_clusters = np.unique(labels)\n",
    "    total_entropy = 0\n",
    "    for cluster in unique_clusters:\n",
    "        indices = np.where(labels == cluster)[0]\n",
    "        class_counts = np.bincount(classes[indices], minlength=np.max(classes) + 1)\n",
    "        cluster_entropy = entropy(class_counts, base=2)\n",
    "        total_entropy += cluster_entropy\n",
    "    return total_entropy / len(unique_clusters)\n",
    "\n",
    "def plot_clusters(embedding_matrix, cluster_labels, original_classes, class_labels = {0: \"Transformed Regular\", 1: \"Patient\"}):\n",
    "\n",
    "    # PCA reduction to 2 dimensions for visualization\n",
    "    pca_2d = TSNE(n_components=2)\n",
    "    spectrograms_pca_2d = pca_2d.fit_transform(embedding_matrix)\n",
    "\n",
    "    # Scatter plot: colored by K-Means clusters\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.scatter(\n",
    "        spectrograms_pca_2d[:, 0],\n",
    "        spectrograms_pca_2d[:, 1],\n",
    "        c=cluster_labels,\n",
    "        cmap='viridis',\n",
    "        alpha=0.7\n",
    "    )\n",
    "    plt.title(\"t-SNE Projection (Colored by K-Means Clusters)\")\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Scatter plot: colored by original classes\n",
    "    # Assuming `original_classes` contains two distinct classes: 0 and 1\n",
    "    colors = ['blue', 'red']  # Colors corresponding to the classes\n",
    "\n",
    "    plt.subplot(1, 2, 2)\n",
    "    scatter = plt.scatter(\n",
    "        spectrograms_pca_2d[:, 0],\n",
    "        spectrograms_pca_2d[:, 1],\n",
    "        c=original_classes,\n",
    "        cmap='coolwarm',\n",
    "        alpha=0.7\n",
    "    )\n",
    "    plt.title(\"t-SNE Projection (Colored by Original Classes)\")\n",
    "    plt.xlabel(\"Component 1\")\n",
    "    plt.ylabel(\"Component 2\")\n",
    "\n",
    "    # Create a legend manually\n",
    "    legend_handles = [\n",
    "        mpatches.Patch(color=color, label=label) for label, color in zip(class_labels.values(), colors)\n",
    "    ]\n",
    "    plt.legend(handles=legend_handles, title=\"Classes\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then define the key preprocessing function (not all of them are used to create our quasi tracheostomy speech) but we provide them for your own experiments:\n",
    "1. `apply_lpc_noise_resynthesis` removes the periodic source of the speech signal.\n",
    "2. `insert_silence` inserts silence segment into the audio segments.\n",
    "3. `add_noise` adds gaussian noise to the audio (not used to generate the quasi tracheostomy speech)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def apply_lpc_noise_resynthesis(input_audio: AudioSegment, radLPC=12) -> AudioSegment:\n",
    "    # Convert AudioSegment to numpy array (mono, float32)\n",
    "    samples = np.array(input_audio.get_array_of_samples()).astype(np.float32)\n",
    "    if input_audio.channels == 2:\n",
    "        samples = samples.reshape((-1, 2))\n",
    "        samples = samples.mean(axis=1)  # Convert to mono\n",
    "\n",
    "    sr = input_audio.frame_rate\n",
    "\n",
    "    # Remove DC component\n",
    "    samples -= np.mean(samples)\n",
    "\n",
    "    # Pre-emphasis\n",
    "    pre_emphasis = 0.8\n",
    "    samples = scipy.signal.lfilter([1, -pre_emphasis], 1, samples)\n",
    "\n",
    "    # Parameters for segmentation\n",
    "    window_length = 512\n",
    "    shift_ratio = 0.5\n",
    "    shift = int(window_length * shift_ratio)\n",
    "\n",
    "    # Hamming window and scaling\n",
    "    window = np.hamming(window_length + 1)[:-1]\n",
    "    constant = np.sum(window) / window_length / shift_ratio\n",
    "    window /= constant\n",
    "\n",
    "    # Output and coefficient placeholders\n",
    "    output = np.zeros(len(samples))\n",
    "    num_segments = (len(samples) - shift) // shift\n",
    "    coefficients = np.full((num_segments, radLPC), np.nan)\n",
    "\n",
    "    segment_index = 0\n",
    "    for i1 in range(0, len(samples) - window_length + 1, shift):\n",
    "        i2 = i1 + window_length\n",
    "        segment = samples[i1:i2] * window\n",
    "\n",
    "        if np.var(segment) > 0:\n",
    "            r = np.correlate(segment, segment, mode='full')[window_length-1:window_length+radLPC]\n",
    "            try:\n",
    "                ar_coeffs = scipy.linalg.solve_toeplitz((r[:-1], r[:-1]), r[1:])\n",
    "                coefficients[segment_index, :len(ar_coeffs)] = ar_coeffs\n",
    "\n",
    "                excitation = np.random.uniform(-1, 1, window_length)\n",
    "                estimated_segment = scipy.signal.lfilter(\n",
    "                    [np.sqrt(np.var(segment))],\n",
    "                    np.concatenate([[1], -ar_coeffs]),\n",
    "                    excitation\n",
    "                )\n",
    "\n",
    "                output[i1:i2] += estimated_segment\n",
    "            except np.linalg.LinAlgError:\n",
    "                pass  # Skip segments that can't be solved due to numerical issues\n",
    "\n",
    "        segment_index += 1\n",
    "\n",
    "    # De-emphasis\n",
    "    output = scipy.signal.lfilter([1], [1, -pre_emphasis], output)\n",
    "\n",
    "    # Normalize\n",
    "    output /= np.max(np.abs(output))\n",
    "\n",
    "    # Convert back to AudioSegment using in-memory buffer\n",
    "    buffer = io.BytesIO()\n",
    "    sf.write(buffer, output, sr, format='WAV')\n",
    "    buffer.seek(0)\n",
    "    return AudioSegment.from_file(buffer, format=\"wav\")\n",
    "\n",
    "def insert_silence(y, sr, num_pauses, length):\n",
    "    \"\"\"\n",
    "    Insert silent segments into an audio signal.\n",
    "    \n",
    "    Args:\n",
    "        y (np.ndarray): The audio signal.\n",
    "        sr (int): The sampling rate of the audio.\n",
    "        num_pauses (int): The number of silent segments to insert.\n",
    "        length (float): The length of each silent segment in seconds.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: The audio signal with silent segments inserted.\n",
    "    \"\"\"\n",
    "    # Calculate the number of samples for the silence\n",
    "    silence_samples = int(length * sr)\n",
    "    silence = np.zeros(silence_samples, dtype=y.dtype)\n",
    "    \n",
    "    '''# Ensure num_pauses does not exceed the possible number of insertion points\n",
    "    max_insertions = len(y) + num_pauses * silence_samples\n",
    "    num_pauses = min(num_pauses, max_insertions // (len(y) + silence_samples))'''\n",
    "    \n",
    "    # Generate random insertion points\n",
    "    insertion_points = np.sort(np.random.choice(len(y), size=num_pauses, replace=False))\n",
    "    \n",
    "    # Insert silence at the specified points\n",
    "    y_with_silence = y.copy()\n",
    "    offset = 0\n",
    "    for point in insertion_points:\n",
    "        point += offset  # Adjust for previously inserted silences\n",
    "        y_with_silence = np.concatenate((y_with_silence[:point], silence, y_with_silence[point:]))\n",
    "        offset += silence_samples\n",
    "    \n",
    "    return y_with_silence\n",
    "\n",
    "def add_noise(y, noise_std = 0.01):\n",
    "    noise = np.random.normal(0, noise_std, y.shape)\n",
    "    return y + noise\n",
    "\n",
    "# OPTIONAL: Add your own preprocessing functions "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment out the Modifications\n",
    "\n",
    "In the cell below, we actually:\n",
    "1. Load regular speech and patient's speech audio objects\n",
    "2. Apply the modifications on the regular speech\n",
    "3. Cluster the modified audios\n",
    "4. Compute the intra-cluster entropy\n",
    "\n",
    "You can specify the modifications yourselves into the `modification` variable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_and_cluster(modification, plot = False, patient_waveforms = None, regular_waveforms = None, **kwargs):\n",
    "    \"\"\"\n",
    "    Process and cluster audio files from two directories.\n",
    "    \n",
    "    Args:\n",
    "        patient_DIR (str): Directory with tracheostomy audios.\n",
    "        regular_DIR (str): Directory with normal voice audios.\n",
    "        \n",
    "    Returns:\n",
    "        float: Average entropy of original classes in each cluster.\n",
    "    \"\"\"\n",
    "    if patient_waveforms is None or regular_waveforms is None:\n",
    "        # Load MP3 files\n",
    "        patient_files = [os.path.join(PATIENT_SEGMENTED_AUDIO_DIR, f) for f in os.listdir(PATIENT_SEGMENTED_AUDIO_DIR)[:60]]\n",
    "        regular_files = [os.path.join(REGULAR_SPEECH_DIR, f) for f in os.listdir(REGULAR_SPEECH_DIR)[:60]]\n",
    "        \n",
    "        # Load waveforms\n",
    "        patient_waveforms = [librosa.load(f, sr=16_000) for f in tqdm(patient_files, desc=\"Loading patient_DIR\")]\n",
    "        regular_waveforms = [librosa.load(f, sr=16_000) for f in tqdm(regular_files, desc=\"Loading regular_DIR\")]\n",
    "        \n",
    "        # Modify the regular speech to sound more like that of the patient\n",
    "        regular_waveforms = [modification(waveform, sr, **kwargs) for waveform, sr in regular_waveforms]\n",
    "        \n",
    "    # Combine all waveforms\n",
    "    all_waveforms = regular_waveforms + patient_waveforms\n",
    "    \n",
    "    # Compute 64 mel spectrograms\n",
    "    with torch.no_grad():\n",
    "        embedding_matrix = []\n",
    "        for w in all_waveforms:\n",
    "            sf.write(\"tmp_sound.wav\", w, 16000)\n",
    "            embedding_matrix.append(inference(\"tmp_sound.wav\")) \n",
    "    embedding_matrix = np.array(embedding_matrix)\n",
    "    print(\"Embedding shape:\",embedding_matrix.shape)\n",
    "    \n",
    "    # K-Means clustering into 2 clusters\n",
    "    kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "    cluster_labels = kmeans.fit_predict(embedding_matrix)\n",
    "    \n",
    "    # Original class labels: 0 for regular_DIR, 1 for PATIENT_DIR\n",
    "    original_classes = np.array([0] * len(regular_waveforms) + [1] * len(patient_waveforms))\n",
    "    if plot: plot_clusters(embedding_matrix, cluster_labels, original_classes)\n",
    "    \n",
    "    # Compute entropy of original classes within each cluster\n",
    "    avg_entropy = compute_entropy(original_classes, cluster_labels)\n",
    "    \n",
    "    return avg_entropy\n",
    "\n",
    "# Example usage\n",
    "\n",
    "# OPTIONAL: Define how the regular speech should be modified to resemble that of our patient\n",
    "modification = lambda waveform, sr, **kwargs: insert_silence(librosa.effects.time_stretch(apply_lpc_noise_resynthesis(waveform), rate = 1/4), sr, np.random.randint(7,12), 0.125) \n",
    "\n",
    "avg_entropy = process_and_cluster(modification, plot = True)\n",
    "print(f\"Average entropy: {avg_entropy}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Refine Modifications by Local Hill Climbing\n",
    "\n",
    "Here we can further refine the results of manual experiments above using a local hill climbing. In order to do that, you have to:\n",
    "\n",
    "1. Use your version of `modification` from above and replace the specific parameter settings by kwargs.\n",
    "2. Set the manually discovered parameter setting into the dictionary `kwargs`.\n",
    "3. In the dictionary `param_ranges` specify ranges of values that should be tried."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def hill_climbing(initial_kwargs, param_ranges, max_iters=100, plot=False):\n",
    "    \"\"\"\n",
    "    Perform hill climbing to maximize intra-cluster entropy.\n",
    "\n",
    "    Args:\n",
    "        initial_kwargs (dict): Initial parameter configuration.\n",
    "        param_ranges (dict): Ranges for each parameter (min, max, step).\n",
    "        max_iters (int): Maximum iterations to run hill climbing.\n",
    "        plot (bool): Whether to plot during clustering.\n",
    "\n",
    "    Returns:\n",
    "        dict: Optimized parameters.\n",
    "        float: Maximum entropy achieved.\n",
    "    \"\"\"\n",
    "    # Initialize parameters and compute initial entropy\n",
    "    current_kwargs = initial_kwargs.copy()\n",
    "    current_entropy = process_and_cluster(PATIENT_SEGMENTED_AUDIO_DIR, REGULAR_SPEECH_DIR, plot=plot, **current_kwargs)\n",
    "\n",
    "    print(f\"Initial entropy: {current_entropy}\")\n",
    "\n",
    "    for iteration in range(max_iters):\n",
    "        neighbors = []\n",
    "\n",
    "        # Generate neighboring configurations by perturbing each parameter\n",
    "        for param, (min_val, max_val, step) in param_ranges.items():\n",
    "            # Increase parameter\n",
    "            if current_kwargs[param] + step <= max_val:\n",
    "                neighbor_up = current_kwargs.copy()\n",
    "                neighbor_up[param] += step\n",
    "                neighbors.append(neighbor_up)\n",
    "\n",
    "            # Decrease parameter\n",
    "            if current_kwargs[param] - step >= min_val:\n",
    "                neighbor_down = current_kwargs.copy()\n",
    "                neighbor_down[param] -= step\n",
    "                neighbors.append(neighbor_down)\n",
    "\n",
    "        # Evaluate neighbors\n",
    "        best_neighbor = current_kwargs\n",
    "        best_entropy = current_entropy\n",
    "\n",
    "        for neighbor in neighbors:\n",
    "            neighbor_entropy = process_and_cluster(PATIENT_SEGMENTED_AUDIO_DIR, REGULAR_SPEECH_DIR, plot=False, **neighbor)\n",
    "\n",
    "            # Update if the neighbor is better\n",
    "            if neighbor_entropy > best_entropy:\n",
    "                best_entropy = neighbor_entropy\n",
    "                best_neighbor = neighbor\n",
    "\n",
    "        # Check if no improvement was found\n",
    "        if best_entropy == current_entropy:\n",
    "            print(f\"No improvement found at iteration {iteration}. Stopping.\")\n",
    "            break\n",
    "\n",
    "        # Update current configuration\n",
    "        current_kwargs = best_neighbor\n",
    "        current_entropy = best_entropy\n",
    "\n",
    "        print(f\"Iteration {iteration + 1}: Current entropy = {current_entropy}\")\n",
    "\n",
    "    return current_kwargs, current_entropy\n",
    "\n",
    "# OPTIONAL: Define how the regular speech should be modified to resemble that of our patient\n",
    "modification = lambda waveform, sr, **kwargs: insert_silence(librosa.effects.time_stretch(apply_lpc_noise_resynthesis(waveform), rate = kwargs['rate']), sr, np.random.randint(kwargs['lower_pauses'],kwargs['higher_pauses']), kwargs['pause_length']) \n",
    "\n",
    "# OPTIONAL: Set the parameter settings from above here\n",
    "kwargs = {\n",
    "    \"rate\": 0.25,\n",
    "    \"lower_pauses\": 7,\n",
    "    \"higher_pauses\": 12,\n",
    "    \"pause_length\": 0.125\n",
    "}\n",
    "\n",
    "# OPTIONAL: Specify the value ranges that will be searched through here\n",
    "# Define parameter ranges: (min, max, step)\n",
    "param_ranges = {\n",
    "    \"rate\": (0.1,1,0.1),\n",
    "    \"lower_pauses\": (5,8,1),\n",
    "    \"higher_pauses\": (10,14,1),\n",
    "    \"pause_length\": (0.125,0.5,0.1)\n",
    "}\n",
    "\n",
    "# Run hill climbing\n",
    "optimized_kwargs, max_entropy = hill_climbing(\n",
    "    kwargs,\n",
    "    param_ranges,\n",
    "    max_iters=50,\n",
    "    plot=True\n",
    ")\n",
    "\n",
    "print(f\"Optimized parameters: {optimized_kwargs}\")\n",
    "print(f\"Maximum entropy: {max_entropy}\")\n",
    "_= process_and_cluster(plot=True, **optimized_kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jasminka",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

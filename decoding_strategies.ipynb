{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decoding\n",
    "\n",
    "In this notebook, we provide an interface to experiment with several decoding strategies and evaluate them.\n",
    "\n",
    "### Setting Up\n",
    "\n",
    "Simply run the cell below to load the libraries, define important functions and set important global ariables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import whisper, torch\n",
    "import numpy as np\n",
    "from data_processing.tokenize_audio import *\n",
    "from tqdm import tqdm\n",
    "import tempfile\n",
    "import torchaudio\n",
    "import torchaudio.transforms as at\n",
    "import os, re, csv\n",
    "from datetime import datetime\n",
    "import ufal.morphodita\n",
    "from pydub import AudioSegment\n",
    "from pydub.silence import split_on_silence\n",
    "from scipy.signal import butter, lfilter\n",
    "from gtts import gTTS\n",
    "from IPython.display import Markdown,Audio, display\n",
    "\n",
    "extract_prefix = lambda s: re.match(r\"[\\wěščřžýáíéóúůĎťňŘŠČŽÝÁÍÉÓÚŮ]+\", s).group(0) if re.match(r\"[\\wěščřžýáíéóúůĎťňŘŠČŽÝÁÍÉÓÚŮ]+\", s) else \"\"\n",
    "\n",
    "\n",
    "# Load the Czech MorphoDiTa lemmatizer model\n",
    "tagger = ufal.morphodita.Tagger.load(\"czech-morfflex/czech-morfflex2.0-pdtc1.0-220710-pos_only.tagger\")\n",
    "\n",
    "def lemmatize_czech(word: str) -> str:\n",
    "    \"\"\"\n",
    "    Takes a Czech word as input and returns its lemma.\n",
    "\n",
    "    Args:\n",
    "        word (str): The input word in Czech.\n",
    "\n",
    "    Returns:\n",
    "        str: The lemmatized form of the word.\n",
    "    \"\"\"\n",
    "    if not tagger:\n",
    "        raise RuntimeError(\"Tagger model failed to load.\")\n",
    "\n",
    "    # Create a tokenizer and tagger\n",
    "    tokenizer = tagger.newTokenizer()\n",
    "    if not tokenizer:\n",
    "        raise RuntimeError(\"Failed to create tokenizer.\")\n",
    "\n",
    "    # Tokenize input\n",
    "    tokenizer.setText(word)\n",
    "    forms = ufal.morphodita.Forms()\n",
    "    lemmas = ufal.morphodita.TaggedLemmas()\n",
    "    tokens = ufal.morphodita.TokenRanges()\n",
    "\n",
    "    while tokenizer.nextSentence(forms,tokens):\n",
    "        tagger.tag(forms, lemmas)\n",
    "\n",
    "    # Return the first lemma (assuming one word input)\n",
    "    return lemmas[0].lemma if lemmas else word  # If no lemma found, return original word\n",
    "\n",
    "def load_wave(wave_path, segment_start, segment_end, sample_rate:int=16000) -> torch.Tensor:\n",
    "    waveform, sr = torchaudio.load(wave_path, normalize=True)\n",
    "    if sample_rate != sr: waveform = at.Resample(sr, sample_rate)(waveform)\n",
    "    segment_of_interest = waveform[:, int(segment_start * sample_rate/1000):int(segment_end * sample_rate/1000)]\n",
    "    if len(segment_of_interest.shape) == 2: segment_of_interest = torch.mean(segment_of_interest, dim = 0)\n",
    "    return segment_of_interest\n",
    "\n",
    "def save_and_load_audiosegment(segment, sample_rate=16000):\n",
    "    with tempfile.NamedTemporaryFile(suffix=\".wav\", delete=False) as tmpfile:\n",
    "        temp_path = tmpfile.name\n",
    "        segment.export(temp_path, format=\"wav\")\n",
    "    \n",
    "    loaded_wave = load_wave(temp_path, 0, len(segment), sample_rate)\n",
    "    os.remove(temp_path)\n",
    "    return loaded_wave\n",
    "\n",
    "MODEL = whisper.load_model(\"small\")\n",
    "checkpoint = torch.load(\"best-checkpoint-epoch=0006-run-small_jasmi_kd5e-2.ckpt\", map_location=\"cpu\")\n",
    "ckpt_state_dict = checkpoint[\"state_dict\"]\n",
    "state_dict = {k.replace(\"model.\",\"\"):v for k,v in ckpt_state_dict.items() if k.startswith(\"model.\")}\n",
    "MODEL.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define Decoding Startegies\n",
    "\n",
    "In the cell below, we define a class `WhisperSpecialistDecoder`. This class takes a Whisper model as input and facilitates an interface for word-by-word transcription strategies that are more sophisticated than what is offered by the iterface of `whisper` library. The strategies implemented so far are:\n",
    "1. **Unguided transcription**: uses a beam search to find N most likely transcriptions.\n",
    "2. **Guided transcription**: uses a beam search to find N most likely transcriptions. The transcriptions are limited to existing Czech words.\n",
    "3. **Guided transcription with merged segments**: uses a beam search to find N most likely transcriptions. The transcriptions are limited to existing Czech words. The true transcription of previous segments is used as well.\n",
    "\n",
    "All of these strategies are implemented via `decode_beam` method. We show how to call each of them further below.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import csv\n",
    "\n",
    "class WhisperSpecialistDecoder:\n",
    "    def __init__(self, vocabulary_source, tokenizer, model, penalty_strength = 0.3):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.model = model\n",
    "        self.transcriptions = []\n",
    "        self.first_token_dict = {}\n",
    "        self.next_token_dict = {}\n",
    "        self.token_lists = []\n",
    "        self.token_lists = []\n",
    "        self.probs_when_unrelated = []\n",
    "        self.penalty_strength = penalty_strength\n",
    "        \n",
    "        # Load CSV and process transcriptions\n",
    "        self._load_csv(vocabulary_source)\n",
    "        self._build_token_dict(first=True)\n",
    "        self._build_token_dict(first=False)\n",
    "        self._build_token_list()\n",
    "    \n",
    "    def _load_csv(self, vocabulary_source):\n",
    "        \"\"\"Loads CSV and extracts first column into transcriptions.\"\"\"\n",
    "        if isinstance(vocabulary_source, str):\n",
    "            with open(vocabulary_source, newline='', encoding='utf-8') as f:\n",
    "                reader = csv.reader(f)\n",
    "                self.transcriptions = [row[0][1:] for row in reader if row]\n",
    "        elif isinstance(vocabulary_source, list):\n",
    "            self.transcriptions = vocabulary_source\n",
    "    \n",
    "    def _build_token_dict(self, first = True):\n",
    "        \"\"\"Encodes transcriptions and builds a nested token dictionary.\"\"\"\n",
    "        for text in self.transcriptions if len(self.transcriptions) < 200 else tqdm(self.transcriptions):\n",
    "            if not first: text = \" \" + text.lower()\n",
    "            tokens = self.tokenizer.encode(f\"{text}<|endoftext|>\", allowed_special = \"all\")\n",
    "            current_dict = self.first_token_dict if first else self.next_token_dict \n",
    "            for token in tokens:\n",
    "                if token in current_dict.keys():\n",
    "                    current_dict = current_dict[token]\n",
    "                else:\n",
    "                    current_dict[token] = {}\n",
    "                    current_dict = current_dict[token]\n",
    "\n",
    "    def _build_token_list(self):\n",
    "        for text in self.transcriptions:\n",
    "            tokens = self.tokenizer.encode(f\"{text}<|endoftext|>\", allowed_special = \"all\") \n",
    "            self.token_lists.append(tokens)\n",
    "\n",
    "    # OPTIONAL: add your own decoding strategies\n",
    "    def decode_beam(self, audio_segment, beam_size=5, n_results = 3, previous_transcription = \"\", guided = True):\n",
    "        \"\"\"\n",
    "        Performs beam search decoding with masked logits, batching all beam hypotheses at once.\n",
    "        \n",
    "        Args:\n",
    "            audio_segment: Input audio waveform.\n",
    "            beam_size: Number of hypotheses to maintain.\n",
    "\n",
    "        Returns:\n",
    "            Best decoded transcription.\n",
    "        \"\"\"\n",
    "        assert beam_size >= n_results, \"N results to be returned cannot exceed the beam size\"\n",
    "        with torch.no_grad():\n",
    "            # Prepare input\n",
    "            audio = whisper.pad_or_trim(audio_segment)\n",
    "            mel = whisper.log_mel_spectrogram(audio, n_mels=80)\n",
    "            encoder_output = self.model.encoder(mel.unsqueeze(0))\n",
    "\n",
    "            # Initialize search\n",
    "            initial_tokens = self.tokenizer.encode(f\"<|startoftranscript|><|cs|><|transcribe|><|notimestamps|>{previous_transcription}\", allowed_special=\"all\")\n",
    "            beam = [(initial_tokens, 0.0, self.next_token_dict if previous_transcription else self.first_token_dict)]  # (tokens, log_prob, current_dict)\n",
    "\n",
    "            # Beam search loop\n",
    "            for e in range(25):  # Max length of sequence\n",
    "                new_beam = []\n",
    "                # Reove finished hypotheses\n",
    "                removed_hypotheses = []\n",
    "                for b, el_tuple in enumerate(beam):\n",
    "                    tokens, _, _ = el_tuple\n",
    "                    if tokens[-1] == self.tokenizer.eot:\n",
    "                        new_beam.append(el_tuple)\n",
    "                        removed_hypotheses.append(b)\n",
    "                for h_idx in removed_hypotheses[::-1]: del beam[h_idx] \n",
    "                if not len(beam):\n",
    "                    beam = new_beam\n",
    "                    break  # Stop early if all hypotheses are finished\n",
    "\n",
    "                # Prepare batched inputs\n",
    "                token_batch = [tokens for tokens, _, _ in beam]\n",
    "                current_dicts = [current_dict for _, _, current_dict in beam]\n",
    "                encoded_txt_tokens = torch.tensor(token_batch).long().to(encoder_output.device)\n",
    "\n",
    "                # Decode batch\n",
    "                logits = self.model.decoder(encoded_txt_tokens, encoder_output)[:, -1, :]  # Shape: (beam_size, vocab_size)\n",
    "\n",
    "                if guided:\n",
    "                    # Apply individual masks for each beam\n",
    "                    vocab_size = logits.shape[-1]\n",
    "                    masks = torch.full((len(beam), vocab_size), float('-inf'), device=logits.device)\n",
    "\n",
    "                    for i, current_dict in enumerate(current_dicts):\n",
    "                        allowed_indices = list(current_dict.keys())\n",
    "                        masks[i, allowed_indices] = 0  # Allow only valid tokens\n",
    "\n",
    "                    logits = logits + masks  # Mask invalid tokens\n",
    "\n",
    "                log_probs = torch.log_softmax(logits, dim=-1)  # Compute log probabilities\n",
    "\n",
    "                # Get top-k candidates for each beam hypothesis\n",
    "                topk_log_probs, topk_indices = torch.topk(log_probs, beam_size, dim=-1)\n",
    "\n",
    "                # Expand beam\n",
    "                for i in range(len(beam)):\n",
    "                    for j in range(beam_size):\n",
    "                        new_token = topk_indices[i, j].item()\n",
    "                        new_log_prob = beam[i][1] + topk_log_probs[i, j].item()\n",
    "                        new_dict = beam[i][2].get(new_token, {})  # Navigate token tree\n",
    "                        new_beam.append((beam[i][0] + [new_token], new_log_prob, new_dict))\n",
    "\n",
    "                # Keep top beam_size hypotheses\n",
    "                beam = sorted(new_beam, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "\n",
    "            # Return best sequence\n",
    "            best_tokens = [(beam_el[0],beam_el[1]) for beam_el in beam[:n_results+1]]  # Best scoring sequence\n",
    "            return [(self.tokenizer.decode(tokens[len(initial_tokens):-1]).split()[0] if self.tokenizer.decode(tokens[len(initial_tokens):-1]).split() else '', log_prob) for tokens,log_prob in best_tokens] #self.tokenizer.decode(best_tokens)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But before we play around with the decoder, we have to load all the Czech words and store them in the decoder."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "czech_word_list = open(\"data/czech_vocabulary.txt\").read().split(\",\")\n",
    "czech_word_list = list(set([word.capitalize() for word in tqdm(czech_word_list) if word]))[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can create our decoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 35116/35116 [00:05<00:00, 6224.53it/s]\n",
      "100%|██████████| 35116/35116 [00:05<00:00, 6153.86it/s]\n"
     ]
    }
   ],
   "source": [
    "woptions = whisper.DecodingOptions(language=\"cs\", without_timestamps=True)\n",
    "tokenizer = whisper.tokenizer.get_tokenizer(True, language=\"cs\", task = woptions.task) \n",
    "decoder = WhisperSpecialistDecoder(czech_word_list,tokenizer, MODEL)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transcribe Conversations\n",
    "\n",
    "For this task we have roecorded three special conversations. In which the patient was asked to keep at least 1s pauses between her individual words.  We load them in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "audio_segment_108 = load_audio(\"data/audio/Jasmi_Mimozemstan_108.mp3\")\n",
    "audio_segment_109 = load_audio(\"data/audio/Jasmi_Hlaseni_109.mp3\")\n",
    "audio_segment_110 = load_audio(\"data/audio/Jasmi_Rytiri_kosmu_110.mp3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we split the audios to the shorter segments corresponding to individual words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def butter_bandpass_filter(data, lowcut, highcut, fs, order=5):\n",
    "    nyquist = 0.5 * fs\n",
    "    low = lowcut / nyquist\n",
    "    high = highcut / nyquist\n",
    "    b, a = butter(order, [low, high], btype='band')\n",
    "    return lfilter(b, a, data)\n",
    "\n",
    "def apply_bandpass(audio: AudioSegment, lowcut=165, highcut=3000):\n",
    "    # Convert audio to NumPy array\n",
    "    samples = np.array(audio.get_array_of_samples(), dtype=np.float32)\n",
    "    sample_rate = audio.frame_rate\n",
    "\n",
    "    # Apply bandpass filter\n",
    "    filtered_samples = butter_bandpass_filter(samples, lowcut, highcut, sample_rate)\n",
    "\n",
    "    # Convert back to int16 (assuming original audio was in 16-bit PCM)\n",
    "    filtered_samples = np.int16(filtered_samples)\n",
    "\n",
    "    # Create new AudioSegment\n",
    "    return AudioSegment(\n",
    "        filtered_samples.tobytes(),\n",
    "        frame_rate=sample_rate,\n",
    "        sample_width=audio.sample_width,\n",
    "        channels=audio.channels\n",
    "    )\n",
    "\n",
    "def extract_loudest_segments(audio: AudioSegment, n_words: int, min_period: int, take_before: int, take_after: int):\n",
    "    \"\"\"\n",
    "    Identifies the n_words loudest 100ms chunks that are at least min_period apart,\n",
    "    and extracts segments around them.\n",
    "\n",
    "    :param audio: AudioSegment to be analyzed\n",
    "    :param n_words: Number of loudest chunks to extract\n",
    "    :param min_period: Minimum distance (ms) between peaks\n",
    "    :param take_before: Time (ms) to include before each peak\n",
    "    :param take_after: Time (ms) to include after each peak\n",
    "    :return: List of AudioSegment chunks\n",
    "    \"\"\"\n",
    "    chunk_size = 100  # 100ms chunks\n",
    "    num_chunks = len(audio) // chunk_size\n",
    "    \n",
    "    # Compute dBFS for each chunk\n",
    "    chunk_loudness = [(i * chunk_size, apply_bandpass(audio[i * chunk_size:(i + 1) * chunk_size]).dBFS) for i in range(num_chunks)]\n",
    "    \n",
    "    # Sort by loudness (descending order)\n",
    "    chunk_loudness.sort(key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Select loudest chunks with min_period constraint\n",
    "    selected_peaks = []\n",
    "    for time, _ in chunk_loudness:\n",
    "        if all(abs(time - p) >= min_period for p in selected_peaks):\n",
    "            selected_peaks.append(time)\n",
    "        if len(selected_peaks) == n_words:\n",
    "            break\n",
    "\n",
    "    # Sort selected peaks by their temporal order\n",
    "    selected_peaks.sort()\n",
    "    \n",
    "    # Extract segments around peaks\n",
    "    chunks = []\n",
    "    for peak in selected_peaks:\n",
    "        start = max(0, peak - take_before)\n",
    "        end = min(len(audio), peak + take_after)\n",
    "        chunks.append(audio[start:end])\n",
    "    \n",
    "    return chunks\n",
    "\n",
    "def get_row_chunks(csv_path, audio_segments):\n",
    "    segment_pointer = 0\n",
    "    last_end = -np.inf\n",
    "\n",
    "    # Storage for all extracted segments\n",
    "    all_extracted_segments = []\n",
    "\n",
    "    with open(csv_path, newline='', encoding='utf-8') as csvfile:\n",
    "        reader = csv.reader(csvfile)\n",
    "        \n",
    "        for row in tqdm(reader):\n",
    "            if len(row) < 3:\n",
    "                continue  # Skip malformed rows\n",
    "            \n",
    "            text = row[0]\n",
    "            start_time = float(row[1])  # Assuming values are in milliseconds\n",
    "            end_time = float(row[2])\n",
    "            if end_time < last_end: segment_pointer += 1\n",
    "            last_end = end_time\n",
    "\n",
    "            \n",
    "            # Count words\n",
    "            word_count = len(text.split())\n",
    "            \n",
    "            # Extract audio segment\n",
    "            short_segment = audio_segments[segment_pointer][start_time:end_time]\n",
    "            \n",
    "            # Get loudest segments\n",
    "            loudest_segments = extract_loudest_segments(short_segment, word_count, 3000, 1500, 2500)\n",
    "            \n",
    "            # Append to results\n",
    "            all_extracted_segments.append(loudest_segments)\n",
    "\n",
    "    return all_extracted_segments\n",
    "\n",
    "all_extracted_segments = []\n",
    "all_extracted_segments = get_row_chunks(\"data/annotations/long_pause_sessions.csv\",[audio_segment_108,audio_segment_109,audio_segment_110])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also load the transcriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path, n_lines = \"data/annotations/long_pause_sessions.csv\",148\n",
    "row_list = []\n",
    "with open(csv_path, newline='', encoding='utf-8') as f:\n",
    "    reader = csv.reader(f)\n",
    "    for row in reader: row_list.append(row)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can finally start transcribing the individual segments. At the very beggining, let's look at transcriptions generated by the provided interface to see why a custom decoder is neccessary  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': 'proces se vyrábějí, kdy se mu podaří',\n",
       " 'segments': [{'id': 0,\n",
       "   'seek': 0,\n",
       "   'start': 0.0,\n",
       "   'end': 4.1,\n",
       "   'text': 'proces se vyrábějí, kdy se mu podaří',\n",
       "   'tokens': [50364,\n",
       "    4318,\n",
       "    887,\n",
       "    369,\n",
       "    371,\n",
       "    6016,\n",
       "    27879,\n",
       "    9648,\n",
       "    73,\n",
       "    870,\n",
       "    11,\n",
       "    350,\n",
       "    3173,\n",
       "    369,\n",
       "    2992,\n",
       "    2497,\n",
       "    64,\n",
       "    15781,\n",
       "    870],\n",
       "   'temperature': 0.0,\n",
       "   'avg_logprob': -0.475648832321167,\n",
       "   'compression_ratio': 0.8541666666666666,\n",
       "   'no_speech_prob': 7.932011709781139e-13}],\n",
       " 'language': 'cs'}"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = MODEL.transcribe(all_extracted_segments[0][0], language = \"cs\")#, beam_size = 110)\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the model returns only one transcription which is insufficient for our needs. Let's furthermore try to look just at the transcription texts of multiple segments: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "silence = AudioSegment.silent(500)\n",
    "for r,row_chunks in enumerate(all_extracted_segments[:15]):\n",
    "    if int(row_list[r][3]) != 0: continue\n",
    "    print(f\"================ {r} ================\")\n",
    "    merged_segment = AudioSegment.empty()\n",
    "    for c, chunk in enumerate(row_chunks): \n",
    "        if c: merged_segment += silence\n",
    "        merged_segment += chunk\n",
    "    whisper_input = save_and_load_audiosegment(merged_segment)\n",
    "    predicted_transcription = MODEL.transcribe(whisper_input, language = \"cs\")[\"text\"]\n",
    "    true_transcription = row_list[c][0]\n",
    "    print(\"Predicted transcription:\", predicted_transcription)\n",
    "    display(Audio(gTTS(text=predicted_transcription, lang = \"cs\", slow = False)))\n",
    "    print(\"True transcription:\", true_transcription)\n",
    "    display(Audio(gTTS(text=true_transcription, lang = \"cs\", slow = False)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's look at the outputs of our own decoder. These are presented in a slighlty different form, because this time, we provide not just one but rather five predictions for every word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word_by_word_transcription(decoding_startegy):\n",
    "    for r, row_chunks in enumerate(all_extracted_segments[:15]):\n",
    "        if int(row_list[r][3]) != 0:continue\n",
    "        # Print the whole true transcription\n",
    "        print(f\"================ {r} ================\")\n",
    "        true_transcription = row_list[r][0]\n",
    "        transcription_words = true_transcription.split()\n",
    "        print(\"True transcription:\", true_transcription)\n",
    "        display(Audio(gTTS(text=true_transcription, lang = \"cs\", slow = False)))\n",
    "        # Loop the words\n",
    "        merged_segment = AudioSegment.empty()\n",
    "        for ch, chunk in enumerate(row_chunks): \n",
    "            merged_segment = AudioSegment.empty()\n",
    "            for c in range(ch + 1): \n",
    "                if c: merged_segment += silence\n",
    "                merged_segment += row_chunks[c]\n",
    "\n",
    "            word_list = decoding_startegy(chunk, merged_segment)\n",
    "            transcription_words\n",
    "            # Display predictions\n",
    "            print(\"True word:\", transcription_words[ch])\n",
    "            print(\"Predictions:\")\n",
    "            for p,pred in enumerate(word_list):\n",
    "                pred_text, pred_log_prob = pred\n",
    "                pred_prob = np.exp(pred_log_prob)\n",
    "                print(f\"{p}. Prediction: {pred_text} (p = {pred_prob})\")\n",
    "\n",
    "# Transcription strategies\n",
    "unguided_transcribe = lambda chunk, merged_segment: decoder.decode_beam(save_and_load_audiosegment(chunk), 15,5,guided=False)\n",
    "guided_transcribe = lambda chunk, merged_segment: decoder.decode_beam(save_and_load_audiosegment(chunk), 15,5,guided=True)\n",
    "guided_and_merging_transcribe = lambda chunk, merged_segment: decoder.decode_beam(save_and_load_audiosegment(merged_segment), 15,5,guided=True)\n",
    "# OPTIONAL: Call/design your own transcription strategies\n",
    "\n",
    "\n",
    "#TODO: replace the undefined variable unselected_strategy by the startegy of your choice\n",
    "word_by_word_transcription(unselected_strategy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantitave Decoding Evaluation\n",
    "\n",
    "Here, we quantitavely evaluate all of our decoding strategies on all usable segments from the imported csv file. As our evaluation metric, we use recall of top N predictions where N ranges from 1 to 10. We lemmatize both predictions and the targets before computing the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def evaluate_recall(strategy):\n",
    "    sentence_list = []\n",
    "    recall = [0 for i in range(5)]\n",
    "    total_chunks = sum([len(line) for line in all_extracted_segments])\n",
    "    silence = AudioSegment.silent(500)\n",
    "\n",
    "    r = 0\n",
    "    start_time = time.time()\n",
    "    for row in tqdm(row_list):\n",
    "        transcription = row[0].split()\n",
    "        recording_type = row[3]\n",
    "        if int(recording_type) != 0: \n",
    "            r += 1\n",
    "            continue\n",
    "        row_chunks = all_extracted_segments[r]\n",
    "        last_sentences = []\n",
    "        for ch,chunk in enumerate(row_chunks):\n",
    "            transcription_word = transcription[ch]\n",
    "            # Get predictions\n",
    "            merged_segment = AudioSegment.empty()\n",
    "            for c in range(ch + 1): \n",
    "                if c: merged_segment += silence\n",
    "                merged_segment += row_chunks[c]\n",
    "\n",
    "            whisper_audio = save_and_load_audiosegment(chunk)\n",
    "            decoded = decoder.decode_beam(whisper_audio,15,9)\n",
    "            # Deterministically preprocess predictions\n",
    "            # Compute the recall\n",
    "            processed_words = [extract_prefix(lemmatize_czech(decoded_word.replace(\" \",\"\"))).replace(\"_\",\"\").capitalize() for decoded_word,_ in decoded]\n",
    "            \n",
    "            for i in range(1,11): \n",
    "                if extract_prefix(lemmatize_czech(transcription_word)).replace(\"_\",\"\").capitalize() in processed_words[:11-i]: \n",
    "                    recall[10-i] += 1/total_chunks\n",
    "\n",
    "        sentence_list.append(last_sentences.copy())\n",
    "        r += 1\n",
    "    end_time = time.time()\n",
    "    second_per_sample = (end_time-start_time)/total_chunks\n",
    "    return second_per_sample, recall\n",
    "\n",
    "# Transcription strategies\n",
    "unguided_transcribe = lambda chunk, merged_segment: decoder.decode_beam(save_and_load_audiosegment(chunk), 15,5,guided=False)\n",
    "guided_transcribe = lambda chunk, merged_segment: decoder.decode_beam(save_and_load_audiosegment(chunk), 15,5,guided=True)\n",
    "guided_and_merging_transcribe = lambda chunk, merged_segment: decoder.decode_beam(save_and_load_audiosegment(merged_segment), 15,5,guided=True)\n",
    "# OPTIONAL: Call/design your own transcription strategies\n",
    "\n",
    "\n",
    "#TODO: replace the undefined variable unselected_strategy by the startegy of your choice\n",
    "second_per_sample, recall = evaluate_recall(unselected_strategy)\n",
    "\n",
    "# Show results\n",
    "for r, recall_item in recall:\n",
    "    print(f\"Top-{r} Recall:\", recall_item)\n",
    "print(\"Seconds per sample:\",second_per_sample)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Jasminka",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
